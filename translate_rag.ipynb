{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{14: {'sections': ['# The Generative AI Application Lifecycle\\n'\n",
      "                   '\\n'\n",
      "                   'An important question for all AI applications is the '\n",
      "                   'relevance of AI features, as AI is a fast evolving field, '\n",
      "                   'to ensure that your application remains relevant, '\n",
      "                   'reliable, and robust, you need to monitor, evaluate, and '\n",
      "                   'improve it continuously. This is where the generative AI '\n",
      "                   'lifecycle comes in.\\n'\n",
      "                   '\\n'\n",
      "                   'The generative AI lifecycle is a framework that guides you '\n",
      "                   'through the stages of developing, deploying, and '\n",
      "                   'maintaining a generative AI application. It helps you to '\n",
      "                   'define your goals, measure your performance, identify your '\n",
      "                   'challenges, and implement your solutions. It also helps '\n",
      "                   'you to align your application with the ethical and legal '\n",
      "                   'standards of your domain and your stakeholders. By '\n",
      "                   'following the generative AI lifecycle, you can ensure that '\n",
      "                   'your application is always delivering value and satisfying '\n",
      "                   'your users.\\n'\n",
      "                   '\\n',\n",
      "                   '## Introduction\\n'\n",
      "                   '\\n'\n",
      "                   'In this chapter, you will:\\n'\n",
      "                   '\\n'\n",
      "                   '- Understand the Paradigm Shift from MLOps to LLMOps\\n'\n",
      "                   '- The LLM Lifecycle\\n'\n",
      "                   '- Lifecycle Tooling\\n'\n",
      "                   '- Lifecycle Metrification and Evaluation\\n'\n",
      "                   '\\n',\n",
      "                   '## Understand the Paradigm Shift from MLOps to LLMOps\\n'\n",
      "                   '\\n'\n",
      "                   'LLMs are a new tool in the Artificial Intelligence '\n",
      "                   'arsenal, they are incredibly powerful in analysis and '\n",
      "                   'generation tasks for applications, however this power has '\n",
      "                   'some consequences in how we streamline AI and Classic '\n",
      "                   'Machine Learning tasks.\\n'\n",
      "                   '\\n'\n",
      "                   'With this, we need a new Paradigm to adapt this tool in a '\n",
      "                   'dynamic, with the correct incentives. We can categorize '\n",
      "                   'older AI apps as \"ML Apps\" and newer AI Apps as \"GenAI '\n",
      "                   'Apps\" or just \"AI Apps\", reflecting the mainstream '\n",
      "                   'technology and techniques used at the time. This shifts '\n",
      "                   'our narrative in multiple ways, look at the following '\n",
      "                   'comparison.\\n'\n",
      "                   '\\n'\n",
      "                   '![LLMOps vs. MLOps '\n",
      "                   'comparison](./images/01-llmops-shift.png?WT.mc_id=academic-105485-koreys)\\n'\n",
      "                   '\\n'\n",
      "                   'Notice that in LLMOps, we are more focused on the App '\n",
      "                   'Developers, using integrations as a key point, using '\n",
      "                   '\"Models-as-a-Service\" and thinking in the following points '\n",
      "                   'for metrics.\\n'\n",
      "                   '\\n'\n",
      "                   '- Quality: Response quality\\n'\n",
      "                   '- Harm: Responsible AI\\n'\n",
      "                   '- Honesty: Response groundedness (Makes sense? It is '\n",
      "                   'correct?)\\n'\n",
      "                   '- Cost: Solution Budget\\n'\n",
      "                   '- Latency: Avg. time for token response\\n'\n",
      "                   '\\n',\n",
      "                   '## The LLM Lifecycle\\n'\n",
      "                   '\\n'\n",
      "                   'First, to understand the lifecycle and the modifications, '\n",
      "                   \"let's note the next infographic.\\n\"\n",
      "                   '\\n'\n",
      "                   '![LLMOps '\n",
      "                   'infographic](./images/02-llmops.png?WT.mc_id=academic-105485-koreys)\\n'\n",
      "                   '\\n'\n",
      "                   'As you may note, this is different from the usual '\n",
      "                   'Lifecycles from MLOps. LLMs have many new requirements, as '\n",
      "                   'Prompting, different techniques to improve quality '\n",
      "                   '(Fine-Tuning, RAG, Meta-Prompts), different assessment and '\n",
      "                   'responsability with responsible AI, lastly, new evaluation '\n",
      "                   'metrics (Quality, Harm, Honesty, Cost and Latency).\\n'\n",
      "                   '\\n'\n",
      "                   'For instance, take a look at how we ideate. Using prompt '\n",
      "                   'engineering to experiment with various LLMs to explore '\n",
      "                   'possibilities to test if their Hypothesis could be '\n",
      "                   'correct.\\n'\n",
      "                   '\\n'\n",
      "                   'Note that this is not linear, but integrated loops, '\n",
      "                   'iterative and with an overarching cycle.\\n'\n",
      "                   '\\n'\n",
      "                   \"How could we explore those steps? Let's step into detail \"\n",
      "                   'in how could we build a lifecycle.\\n'\n",
      "                   '\\n'\n",
      "                   '![LLMOps '\n",
      "                   'Workflow](./images/03-llm-stage-flows.png?WT.mc_id=academic-105485-koreys)\\n'\n",
      "                   '\\n'\n",
      "                   'This may look a bit complicated, lets focus on the three '\n",
      "                   'big steps first.\\n'\n",
      "                   '\\n'\n",
      "                   '1. Ideating/Exploring: Exploration, here we can explore '\n",
      "                   'according to our business needs. Prototyping, creating a '\n",
      "                   '[PromptFlow](https://microsoft.github.io/promptflow/index.html?WT.mc_id=academic-105485-koreyst) '\n",
      "                   'and test if is efficient enough for our Hypothesis.\\n'\n",
      "                   '1. Building/Augmenting: Implementation, now, we start to '\n",
      "                   'evaluate for bigger datasets implement techniques, like '\n",
      "                   'Fine-tuning and RAG, to check the robustness of our '\n",
      "                   'solution. If it does not, re-implementing it, adding new '\n",
      "                   'steps in our flow or restructuring the data, might help. '\n",
      "                   'After testing our flow and our scale, if it works and '\n",
      "                   'check our Metrics, it is ready for the next step.\\n'\n",
      "                   '1. Operationalizing: Integration, now adding Monitoring '\n",
      "                   'and Alerts Systems to our system, deployment and '\n",
      "                   'application integration to our Application.\\n'\n",
      "                   '\\n'\n",
      "                   'Then, we have the overarching cycle of Management, '\n",
      "                   'focusing on security, compliance and governance.\\n'\n",
      "                   '\\n'\n",
      "                   'Congratulations, now you have your AI App ready to go and '\n",
      "                   'operational. For a hands on experience, take a look on the '\n",
      "                   '[Contoso Chat '\n",
      "                   'Demo.](https://nitya.github.io/contoso-chat/?WT.mc_id=academic-105485-koreys)\\n'\n",
      "                   '\\n'\n",
      "                   'Now, what tools could we use?\\n'\n",
      "                   '\\n',\n",
      "                   '## Lifecycle Tooling\\n'\n",
      "                   '\\n'\n",
      "                   'For Tooling, Microsoft provides the [Azure AI '\n",
      "                   'Platform](https://azure.microsoft.com/solutions/ai/?WT.mc_id=academic-105485-koreys) '\n",
      "                   'and '\n",
      "                   '[PromptFlow](https://microsoft.github.io/promptflow/index.html?WT.mc_id=academic-105485-koreyst) '\n",
      "                   'facilitate and make your cycle easy to implement and ready '\n",
      "                   'to go.\\n'\n",
      "                   '\\n'\n",
      "                   'The [Azure AI '\n",
      "                   'Platform](https://azure.microsoft.com/solutions/ai/?WT.mc_id=academic-105485-koreys), '\n",
      "                   'allows you to use [AI '\n",
      "                   'Studio](https://ai.azure.com/?WT.mc_id=academic-105485-koreys). '\n",
      "                   'AI Studio is a web portal allows you to Explore models, '\n",
      "                   'samples and tools. Managing your resources, UI development '\n",
      "                   'flows and SDK/CLI options for Code-First development.\\n'\n",
      "                   '\\n'\n",
      "                   '![Azure AI '\n",
      "                   'possibilities](./images/04-azure-ai-platform.png?WT.mc_id=academic-105485-koreys)\\n'\n",
      "                   '\\n'\n",
      "                   'Azure AI, allows you to use multiple resources, to manage '\n",
      "                   'your operations, services, projects, vector search and '\n",
      "                   'databases needs.\\n'\n",
      "                   '\\n'\n",
      "                   '![LLMOps with Azure '\n",
      "                   'AI](./images/05-llm-azure-ai-prompt.png?WT.mc_id=academic-105485-koreys)\\n'\n",
      "                   '\\n'\n",
      "                   'Construct, from Proof-of-Concept(POC) until large scale '\n",
      "                   'applications with PromptFlow:\\n'\n",
      "                   '\\n'\n",
      "                   '- Design and Build apps from VS Code, with visual and '\n",
      "                   'functional tools\\n'\n",
      "                   '- Test and fine-tune your apps for quality AI, with ease.\\n'\n",
      "                   '- Use Azure AI Studio to Integrate and Iterate with cloud, '\n",
      "                   'Push and Deploy for quick integration.\\n'\n",
      "                   '\\n'\n",
      "                   '![LLMOps with '\n",
      "                   'PromptFlow](./images/06-llm-promptflow.png?WT.mc_id=academic-105485-koreys)\\n'\n",
      "                   '\\n',\n",
      "                   '## Great! Continue your Learning!\\n'\n",
      "                   '\\n'\n",
      "                   'Amazing, now learn more about how we structure an '\n",
      "                   'application to use the concepts with the [Contoso Chat '\n",
      "                   'App](https://nitya.github.io/contoso-chat/?WT.mc_id=academic-105485-koreyst), '\n",
      "                   'to check how Cloud Advocacy adds those concepts in '\n",
      "                   'demonstrations. For more content, check our [Ignite '\n",
      "                   'breakout session!\\n'\n",
      "                   '](https://www.youtube.com/watch?v=DdOylyrTOWg)\\n'\n",
      "                   '\\n'\n",
      "                   'Now, check Lesson 15, to understand how [Retrieval '\n",
      "                   'Augmented Generation and Vector '\n",
      "                   'Databases](../15-rag-and-vector-databases/README.md?WT.mc_id=academic-105485-koreyst) '\n",
      "                   'impact Generative AI and to make more engaging '\n",
      "                   'Applications!\\n'],\n",
      "      'title': 'the-generative-ai-application-lifecycle'}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def extract_info(file_path):\n",
    "    # Use regex to match the first two digits and the title\n",
    "    match = re.match(r\"(\\d{2})-([\\w-]+)\", file_path)\n",
    "\n",
    "    # If a match is found, extract the digits and the title\n",
    "    if match:\n",
    "        digits = int(match.group(1))  # Convert digits to integer\n",
    "        title = match.group(2)\n",
    "        return digits, title\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# 1. 각 파일을 읽고\n",
    "def read_readme_from_directories():\n",
    "    dir_pattern = r\"^\\d{2}-\"\n",
    "\n",
    "    result = {}\n",
    "    for dir_name in os.listdir():\n",
    "        if os.path.isdir(os.path.join(dir_name)) and re.match(dir_pattern, dir_name):\n",
    "            readme_path = os.path.join(dir_name, \"README.md\")\n",
    "            readme_path_ko = os.path.join(dir_name, \"translations/ko/README.md\")\n",
    "            if not os.path.exists(readme_path_ko):\n",
    "                with open(readme_path, \"r\", encoding=\"utf-8\") as readme_file:\n",
    "                    content = readme_file.read()\n",
    "                digits, title = extract_info(dir_name)\n",
    "                result[digits] = {\n",
    "                    \"title\": title,\n",
    "                    \"sections\": split_markdown(content),\n",
    "                }\n",
    "    return result\n",
    "\n",
    "\n",
    "# 2. 헤딩 (#, ##, ### …) 별로 나눈 뒤\n",
    "def split_markdown(content):\n",
    "    pattern = r\"^(#+\\s)\"\n",
    "    sections = re.split(pattern, content, flags=re.MULTILINE)[1:]\n",
    "    return [sections[i] + sections[i + 1] for i in range(0, len(sections), 2)]\n",
    "\n",
    "\n",
    "readme_files = read_readme_from_directories()\n",
    "pprint(readme_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.12/site-packages (1.0.1)\n",
      "Requirement already satisfied: openai in ./.venv/lib/python3.12/site-packages (1.48.0)\n",
      "Requirement already satisfied: pinecone in ./.venv/lib/python3.12/site-packages (5.3.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.12/site-packages (from openai) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.12/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.12/site-packages (from openai) (0.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.venv/lib/python3.12/site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./.venv/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in ./.venv/lib/python3.12/site-packages (from pinecone) (2024.8.30)\n",
      "Requirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from pinecone) (1.1.0)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in ./.venv/lib/python3.12/site-packages (from pinecone) (0.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in ./.venv/lib/python3.12/site-packages (from pinecone) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in ./.venv/lib/python3.12/site-packages (from pinecone) (2.2.3)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in ./.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.5.3->pinecone) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv openai pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_file(file_path, content):\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jpark/dev/generative-ai-for-beginners/.venv/lib/python3.12/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for translation of: # The Generative AI Application Lifecycle\n",
      "\n",
      "An important question for all AI applications is the relevance of AI features, as AI is a fast evolving field, to ensure that your application remains relevant, reliable, and robust, you need to monitor, evaluate, and improve it continuously. This is where the generative AI lifecycle comes in.\n",
      "\n",
      "The generative AI lifecycle is a framework that guides you through the stages of developing, deploying, and maintaining a generative AI application. It helps you to define your goals, measure your performance, identify your challenges, and implement your solutions. It also helps you to align your application with the ethical and legal standards of your domain and your stakeholders. By following the generative AI lifecycle, you can ensure that your application is always delivering value and satisfying your users.\n",
      "\n",
      "...\n",
      "\n",
      "Searching for translation of: ## Introduction\n",
      "\n",
      "In this chapter, you will:\n",
      "\n",
      "- Understand the Paradigm Shift from MLOps to LLMOps\n",
      "- The LLM Lifecycle\n",
      "- Lifecycle Tooling\n",
      "- Lifecycle Metrification and Evaluation\n",
      "\n",
      "...\n",
      "\n",
      "Searching for translation of: ## Understand the Paradigm Shift from MLOps to LLMOps\n",
      "\n",
      "LLMs are a new tool in the Artificial Intelligence arsenal, they are incredibly powerful in analysis and generation tasks for applications, however this power has some consequences in how we streamline AI and Classic Machine Learning tasks.\n",
      "\n",
      "With this, we need a new Paradigm to adapt this tool in a dynamic, with the correct incentives. We can categorize older AI apps as \"ML Apps\" and newer AI Apps as \"GenAI Apps\" or just \"AI Apps\", reflecting the mainstream technology and techniques used at the time. This shifts our narrative in multiple ways, look at the following comparison.\n",
      "\n",
      "![LLMOps vs. MLOps comparison](./images/01-llmops-shift.png?WT.mc_id=academic-105485-koreys)\n",
      "\n",
      "Notice that in LLMOps, we are more focused on the App Developers, using integrations as a key point, using \"Models-as-a-Service\" and thinking in the following points for metrics.\n",
      "\n",
      "- Quality: Response quality\n",
      "- Harm: Responsible AI\n",
      "- Honesty: Response groundedness (Makes sense? It is correct?)\n",
      "- Cost: Solution Budget\n",
      "- Latency: Avg. time for token response\n",
      "\n",
      "...\n",
      "\n",
      "Searching for translation of: ## The LLM Lifecycle\n",
      "\n",
      "First, to understand the lifecycle and the modifications, let's note the next infographic.\n",
      "\n",
      "![LLMOps infographic](./images/02-llmops.png?WT.mc_id=academic-105485-koreys)\n",
      "\n",
      "As you may note, this is different from the usual Lifecycles from MLOps. LLMs have many new requirements, as Prompting, different techniques to improve quality (Fine-Tuning, RAG, Meta-Prompts), different assessment and responsability with responsible AI, lastly, new evaluation metrics (Quality, Harm, Honesty, Cost and Latency).\n",
      "\n",
      "For instance, take a look at how we ideate. Using prompt engineering to experiment with various LLMs to explore possibilities to test if their Hypothesis could be correct.\n",
      "\n",
      "Note that this is not linear, but integrated loops, iterative and with an overarching cycle.\n",
      "\n",
      "How could we explore those steps? Let's step into detail in how could we build a lifecycle.\n",
      "\n",
      "![LLMOps Workflow](./images/03-llm-stage-flows.png?WT.mc_id=academic-105485-koreys)\n",
      "\n",
      "This may look a bit complicated, lets focus on the three big steps first.\n",
      "\n",
      "1. Ideating/Exploring: Exploration, here we can explore according to our business needs. Prototyping, creating a [PromptFlow](https://microsoft.github.io/promptflow/index.html?WT.mc_id=academic-105485-koreyst) and test if is efficient enough for our Hypothesis.\n",
      "1. Building/Augmenting: Implementation, now, we start to evaluate for bigger datasets implement techniques, like Fine-tuning and RAG, to check the robustness of our solution. If it does not, re-implementing it, adding new steps in our flow or restructuring the data, might help. After testing our flow and our scale, if it works and check our Metrics, it is ready for the next step.\n",
      "1. Operationalizing: Integration, now adding Monitoring and Alerts Systems to our system, deployment and application integration to our Application.\n",
      "\n",
      "Then, we have the overarching cycle of Management, focusing on security, compliance and governance.\n",
      "\n",
      "Congratulations, now you have your AI App ready to go and operational. For a hands on experience, take a look on the [Contoso Chat Demo.](https://nitya.github.io/contoso-chat/?WT.mc_id=academic-105485-koreys)\n",
      "\n",
      "Now, what tools could we use?\n",
      "\n",
      "...\n",
      "\n",
      "Searching for translation of: ## Lifecycle Tooling\n",
      "\n",
      "For Tooling, Microsoft provides the [Azure AI Platform](https://azure.microsoft.com/solutions/ai/?WT.mc_id=academic-105485-koreys) and [PromptFlow](https://microsoft.github.io/promptflow/index.html?WT.mc_id=academic-105485-koreyst) facilitate and make your cycle easy to implement and ready to go.\n",
      "\n",
      "The [Azure AI Platform](https://azure.microsoft.com/solutions/ai/?WT.mc_id=academic-105485-koreys), allows you to use [AI Studio](https://ai.azure.com/?WT.mc_id=academic-105485-koreys). AI Studio is a web portal allows you to Explore models, samples and tools. Managing your resources, UI development flows and SDK/CLI options for Code-First development.\n",
      "\n",
      "![Azure AI possibilities](./images/04-azure-ai-platform.png?WT.mc_id=academic-105485-koreys)\n",
      "\n",
      "Azure AI, allows you to use multiple resources, to manage your operations, services, projects, vector search and databases needs.\n",
      "\n",
      "![LLMOps with Azure AI](./images/05-llm-azure-ai-prompt.png?WT.mc_id=academic-105485-koreys)\n",
      "\n",
      "Construct, from Proof-of-Concept(POC) until large scale applications with PromptFlow:\n",
      "\n",
      "- Design and Build apps from VS Code, with visual and functional tools\n",
      "- Test and fine-tune your apps for quality AI, with ease.\n",
      "- Use Azure AI Studio to Integrate and Iterate with cloud, Push and Deploy for quick integration.\n",
      "\n",
      "![LLMOps with PromptFlow](./images/06-llm-promptflow.png?WT.mc_id=academic-105485-koreys)\n",
      "\n",
      "...\n",
      "\n",
      "Searching for translation of: ## Great! Continue your Learning!\n",
      "\n",
      "Amazing, now learn more about how we structure an application to use the concepts with the [Contoso Chat App](https://nitya.github.io/contoso-chat/?WT.mc_id=academic-105485-koreyst), to check how Cloud Advocacy adds those concepts in demonstrations. For more content, check our [Ignite breakout session!\n",
      "](https://www.youtube.com/watch?v=DdOylyrTOWg)\n",
      "\n",
      "Now, check Lesson 15, to understand how [Retrieval Augmented Generation and Vector Databases](../15-rag-and-vector-databases/README.md?WT.mc_id=academic-105485-koreyst) impact Generative AI and to make more engaging Applications!\n",
      "...\n",
      "\n",
      "its query: {'id': '7-28',\n",
      " 'metadata': {'en': '## Great Work! Continue Your Learning\\n'\n",
      "                    '\\n'\n",
      "                    'After completing this lesson, check out our [Generative '\n",
      "                    'AI Learning '\n",
      "                    'collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) '\n",
      "                    'to continue leveling up your Generative AI knowledge!\\n'\n",
      "                    '\\n'\n",
      "                    'Head over to Lesson 7 where we will look at how to [build '\n",
      "                    'chat '\n",
      "                    'applications](../07-building-chat-applications/README.md?WT.mc_id=academic-105485-koreyst)!\\n',\n",
      "              'ko': '## 수고하셨습니다! 학습을 계속하세요\\n'\n",
      "                    '\\n'\n",
      "                    '이 수업을 완료한 후에는 [Generative AI 학습 '\n",
      "                    '컬렉션](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst)을 '\n",
      "                    '확인하여 Generative AI 지식을 더욱 향상시킬 수 있습니다!\\n'\n",
      "                    '\\n'\n",
      "                    '7번 수업으로 이동하여 [채팅 애플리케이션을 구축하는 '\n",
      "                    '방법](../../../07-building-chat-applications/translations/ko/README.md?WT.mc_id=academic-105485-koreyst)을 '\n",
      "                    '살펴보세요!\\n'},\n",
      " 'score': 0.786292374,\n",
      " 'values': []}\n",
      "\n",
      "its query: {'id': '12-0',\n",
      " 'metadata': {'en': '# Building Generative AI-Powered Chat Applications\\n'\n",
      "                    '\\n'\n",
      "                    '[![Building Generative AI-Powered Chat '\n",
      "                    'Applications](./images/07-lesson-banner.png?WT.mc_id=academic-105485-koreyst)](https://aka.ms/gen-ai-lessons7-gh?WT.mc_id=academic-105485-koreyst)\\n'\n",
      "                    '\\n'\n",
      "                    '> _(Click the image above to view video of this lesson)_\\n'\n",
      "                    '\\n'\n",
      "                    \"Now that we've seen how we can build text-generation \"\n",
      "                    \"apps, let's look into chat applications.\\n\"\n",
      "                    '\\n'\n",
      "                    'Chat applications have become integrated into our daily '\n",
      "                    'lives, offering more than just a means of casual '\n",
      "                    \"conversation. They're integral parts of customer service, \"\n",
      "                    'technical support, and even sophisticated advisory '\n",
      "                    \"systems. It's likely that you've gotten some help from a \"\n",
      "                    'chat application not too long ago. As we integrate more '\n",
      "                    'advanced technologies like generative AI into these '\n",
      "                    'platforms, the complexity increases and so does the '\n",
      "                    'challenges.\\n'\n",
      "                    '\\n'\n",
      "                    'Some questions we need to be answered are:\\n'\n",
      "                    '\\n'\n",
      "                    '- **Building the app**. How do we efficiently build and '\n",
      "                    'seamlessly integrate these AI-powered applications for '\n",
      "                    'specific use cases?\\n'\n",
      "                    '- **Monitoring**. Once deployed, how can we monitor and '\n",
      "                    'ensure that the applications are operating at the highest '\n",
      "                    'level of quality, both in terms of functionality and '\n",
      "                    'adhering to the [six principles of responsible '\n",
      "                    'AI](https://www.microsoft.com/ai/responsible-ai?WT.mc_id=academic-105485-koreyst)?\\n'\n",
      "                    '\\n'\n",
      "                    'As we move further into an age defined by automation and '\n",
      "                    'seamless human-machine interactions, understanding how '\n",
      "                    'generative AI transforms the scope, depth, and '\n",
      "                    'adaptability of chat applications becomes essential. This '\n",
      "                    'lesson will investigate the aspects of architecture that '\n",
      "                    'support these intricate systems, delve into the '\n",
      "                    'methodologies for fine-tuning them for domain-specific '\n",
      "                    'tasks, and evaluate the metrics and considerations '\n",
      "                    'pertinent to ensuring responsible AI deployment.\\n'\n",
      "                    '\\n',\n",
      "              'ko': '# 생성형 AI를 활용한 채팅 애플리케이션 구축\\n'\n",
      "                    '\\n'\n",
      "                    '[![Generative AI를 활용한 채팅 애플리케이션 '\n",
      "                    '구축](../../images/07-lesson-banner.png?WT.mc_id=academic-105485-koreyst)](https://aka.ms/gen-ai-lessons7-gh?WT.mc_id=academic-105485-koreyst)\\n'\n",
      "                    '\\n'\n",
      "                    '> _(위의 이미지를 클릭하여 이 레슨의 비디오를 시청하세요)_\\n'\n",
      "                    '\\n'\n",
      "                    '텍스트 생성 앱을 구축하는 방법을 살펴보았으니 이제 채팅 애플리케이션에 대해 알아보겠습니다.\\n'\n",
      "                    '\\n'\n",
      "                    '채팅 애플리케이션은 우리의 일상에 통합되어 있으며, 단순한 대화 수단 이상을 제공합니다. 고객 서비스, '\n",
      "                    '기술 지원, 심지어 정교한 상담 시스템의 핵심 요소입니다. 얼마 전에도 채팅 애플리케이션으로부터 도움을 '\n",
      "                    '받은 적이 있을 것입니다. 우리가 이러한 플랫폼에 생성형 AI와 같은 고급 기술을 통합할수록 복잡성과 '\n",
      "                    '도전 과제도 증가합니다.\\n'\n",
      "                    '\\n'\n",
      "                    '다음과 같은 몇 가지 질문에 대답해야 합니다:\\n'\n",
      "                    '\\n'\n",
      "                    '- **앱 구축**. 특정 사용 사례에 대해 이 AI 기반 애플리케이션을 효율적으로 구축하고 원활하게 '\n",
      "                    '통합하는 방법은 무엇인가요?\\n'\n",
      "                    '- **모니터링**. 배포된 후에는 기능과 [책임 있는 AI의 여섯 가지 '\n",
      "                    '원칙](https://www.microsoft.com/ai/responsible-ai?WT.mc_id=academic-105485-koreyst)을 '\n",
      "                    '준수하는 채로 애플리케이션이 최고 수준의 품질로 작동하는지 어떻게 모니터링할 수 있을까요?\\n'\n",
      "                    '\\n'\n",
      "                    '자동화와 원활한 인간-기계 상호작용으로 정의되는 시대로 나아감에 따라, 생성형 AI가 채팅 '\n",
      "                    '애플리케이션의 범위, 깊이 및 적응성을 어떻게 변화시키는지 이해하는 것이 필수적입니다. 이 레슨에서는 '\n",
      "                    '이러한 복잡한 시스템을 지원하는 아키텍처의 측면을 조사하고, 도메인 특정 작업에 대해 세밀하게 조정하는 '\n",
      "                    '방법을 탐구하며, 책임 있는 AI 배포를 보장하기 위한 지표와 고려 사항을 평가할 것입니다.\\n'\n",
      "                    '\\n'},\n",
      " 'score': 0.654973,\n",
      " 'values': []}\n",
      "\n",
      "its query: {'id': '6-0',\n",
      " 'metadata': {'en': '# Exploring and comparing different LLMs\\n'\n",
      "                    '\\n'\n",
      "                    '[![Exploring and comparing different '\n",
      "                    'LLMs](./images/02-lesson-banner.png?WT.mc_id=academic-105485-koreyst)](https://aka.ms/gen-ai-lesson2-gh?WT.mc_id=academic-105485-koreyst)\\n'\n",
      "                    '\\n'\n",
      "                    '> _Click the image above to view video of this lesson_\\n'\n",
      "                    '\\n'\n",
      "                    'With the previous lesson, we have seen how Generative AI '\n",
      "                    'is changing the technology landscape, how Large Language '\n",
      "                    'Models (LLMs) work and how a business - like our startup '\n",
      "                    '- can apply them to their use cases and grow! In this '\n",
      "                    \"chapter, we're looking to compare and contrast different \"\n",
      "                    'types of large language models (LLMs) to understand their '\n",
      "                    'pros and cons.\\n'\n",
      "                    '\\n'\n",
      "                    \"The next step in our startup's journey is exploring the \"\n",
      "                    'current landscape of LLMs and understanding which are '\n",
      "                    'suitable for our use case.\\n'\n",
      "                    '\\n',\n",
      "              'ko': '# 다양한 LLM 탐색과 비교\\n'\n",
      "                    '\\n'\n",
      "                    '[![Exploring and comparing different '\n",
      "                    'LLMs](../../images/02-lesson-banner.png?WT.mc_id=academic-105485-koreyst)](https://aka.ms/gen-ai-lesson2-gh?WT.mc_id=academic-105485-koreyst)\\n'\n",
      "                    '\\n'\n",
      "                    '> _위의 이미지를 클릭하여 이 레슨의 비디오를 시청하세요._\\n'\n",
      "                    '\\n'\n",
      "                    '이전 레슨에서 우리는 생성형 AI가 기술적인 환경을 변화시키고, 대형 언어 모델 (LLM)이 어떻게 '\n",
      "                    '작동하는지, 그리고 우리 스타트업과 같은 비즈니스가 그들을 사용하여 사용 사례를 적용하고 성장할 수 '\n",
      "                    '있는지를 보았습니다. 이 장에서는 다른 유형의 대형 언어 모델인 LLM을 비교하고 대조하여 그들의 '\n",
      "                    '장단점을 이해하기 위해 탐색할 것입니다.\\n'\n",
      "                    '\\n'\n",
      "                    '우리 스타트업의 여정에서 다음 단계는 현재의 대형 언어 모델 (LLM) 환경을 탐색하고 우리의 사용 '\n",
      "                    '사례에 적합한 모델을 이해하는 것입니다.\\n'\n",
      "                    '\\n'},\n",
      " 'score': 0.629349,\n",
      " 'values': []}\n",
      "\n",
      "its query: {'id': '6-10',\n",
      " 'metadata': {'en': '## Improving LLM results\\n'\n",
      "                    '\\n'\n",
      "                    'We’ve explored with our startup team different kinds of '\n",
      "                    'LLMs and a Cloud Platform (Azure Machine Learning) '\n",
      "                    'enabling us to compare different models, evaluate them on '\n",
      "                    'test data, improve performance and deploy them on '\n",
      "                    'inference endpoints.\\n'\n",
      "                    '\\n'\n",
      "                    'But when shall they consider fine-tuning a model rather '\n",
      "                    'than using a pre-trained one? Are there other approaches '\n",
      "                    'to improve model performance on specific workloads?\\n'\n",
      "                    '\\n'\n",
      "                    'There are several approaches a business can use to get '\n",
      "                    'the results they need from an LLM. You can select '\n",
      "                    'different types of models with different degrees of '\n",
      "                    'training when deploying an LLM in production, with '\n",
      "                    'different levels of complexity, cost, and quality. Here '\n",
      "                    'are some different approaches:\\n'\n",
      "                    '\\n'\n",
      "                    '- **Prompt engineering with context**. The idea is to '\n",
      "                    'provide enough context when you prompt to ensure you get '\n",
      "                    'the responses you need.\\n'\n",
      "                    '\\n'\n",
      "                    '- **Retrieval Augmented Generation, RAG**. Your data '\n",
      "                    'might exist in a database or web endpoint for example, to '\n",
      "                    'ensure this data, or a subset of it, is included at the '\n",
      "                    'time of prompting, you can fetch the relevant data and '\n",
      "                    \"make that part of the user's prompt.\\n\"\n",
      "                    '\\n'\n",
      "                    '- **Fine-tuned model**. Here, you trained the model '\n",
      "                    'further on your own data which leads to the model being '\n",
      "                    'more exact and responsive to your needs but might be '\n",
      "                    'costly.\\n'\n",
      "                    '\\n'\n",
      "                    '![LLMs '\n",
      "                    'deployment](./images/Deploy.png?WT.mc_id=academic-105485-koreyst)\\n'\n",
      "                    '\\n'\n",
      "                    'Img source: [Four Ways that Enterprises Deploy LLMs | '\n",
      "                    'Fiddler AI '\n",
      "                    'Blog](https://www.fiddler.ai/blog/four-ways-that-enterprises-deploy-llms?WT.mc_id=academic-105485-koreyst)\\n'\n",
      "                    '\\n',\n",
      "              'ko': '## LLM 결과 개선하기\\n'\n",
      "                    '\\n'\n",
      "                    '우리 스타트업 팀은 다양한 종류의 LLM과 Cloud Platform (Azure Machine '\n",
      "                    'Learning)을 탐색하여 다른 모델을 비교하고, 테스트 데이터로 평가하고, 성능을 개선하고, 추론 '\n",
      "                    '엔드포인트에 배포할 수 있는 기능을 활용했습니다.\\n'\n",
      "                    '\\n'\n",
      "                    '그러나 언제 모델을 fine-tuning하여 사전 훈련된 모델 대신 사용해야 할까요? 특정 작업 '\n",
      "                    '부하에서 모델 성능을 개선하기 위한 다른 접근 방식이 있을까요?\\n'\n",
      "                    '\\n'\n",
      "                    '비즈니스가 LLM에서 필요한 결과를 얻기 위해 사용할 수 있는 여러 가지 접근 방식이 있습니다. 다양한 '\n",
      "                    '수준의 훈련을 거친 다른 유형의 모델을 선택할 수 있으며, 복잡성, 비용 및 품질이 다른 LLM을 '\n",
      "                    '프로덕션에 배포할 수 있습니다. 다음은 몇 가지 다른 접근 방식입니다:\\n'\n",
      "                    '\\n'\n",
      "                    '- **컨텍스트를 고려한 프롬프트 엔지니어링**. 프롬프트할 때 충분한 컨텍스트를 제공하여 필요한 '\n",
      "                    '응답을 얻을 수 있도록 하는 것입니다.\\n'\n",
      "                    '\\n'\n",
      "                    '- **검색 증강 생성 (Retrieval Augmented Generation), RAG**. '\n",
      "                    '데이터가 데이터베이스나 웹 엔드포인트에 존재할 수 있습니다. 프롬프트할 때 해당 데이터 또는 그 일부를 '\n",
      "                    '포함하도록하여 관련 데이터를 가져와 사용자의 프롬프트의 일부로 만들 수 있습니다.\\n'\n",
      "                    '\\n'\n",
      "                    '- **파인튜닝 모델**. 여기서는 자체 데이터로 모델을 추가로 훈련시켜 모델이 더 정확하고 사용자의 '\n",
      "                    '요구에 더 잘 대응하지만 비용이 발생할 수 있습니다.\\n'\n",
      "                    '\\n'\n",
      "                    '![LLMs '\n",
      "                    'deployment](../../images/Deploy.png?WT.mc_id=academic-105485-koreyst)\\n'\n",
      "                    '\\n'\n",
      "                    '이미지 출처: [Four Ways that Enterprises Deploy LLMs | Fiddler '\n",
      "                    'AI '\n",
      "                    'Blog](https://www.fiddler.ai/blog/four-ways-that-enterprises-deploy-llms?WT.mc_id=academic-105485-koreyst)\\n'\n",
      "                    '\\n'},\n",
      " 'score': 0.691096961,\n",
      " 'values': []}\n",
      "\n",
      "its query: {'id': '6-9',\n",
      " 'metadata': {'en': '## How to test and iterate with different models to '\n",
      "                    'understand performance on Azure\\n'\n",
      "                    '\\n'\n",
      "                    'Once our team has explored the current LLMs landscape and '\n",
      "                    'identified some good candidates for their scenarios, the '\n",
      "                    'next step is testing them on their data and on their '\n",
      "                    'workload. This is an iterative process, done by '\n",
      "                    'experiments and measures.\\n'\n",
      "                    'Most of the models we mentioned in previous paragraphs '\n",
      "                    '(OpenAI models, open source models like Llama2, and '\n",
      "                    'Hugging Face transformers) are available in the [Model '\n",
      "                    'Catalog](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview?WT.mc_id=academic-105485-koreyst) '\n",
      "                    'in [Azure AI '\n",
      "                    'Studio](https://ai.azure.com/?WT.mc_id=academic-105485-koreyst).\\n'\n",
      "                    '\\n'\n",
      "                    '[Azure AI '\n",
      "                    'Studio](https://learn.microsoft.com/azure/ai-studio/what-is-ai-studio?WT.mc_id=academic-105485-koreyst) '\n",
      "                    'is a Cloud Platform designed for developers to build '\n",
      "                    'generative AI applications and manage the whole '\n",
      "                    'development lifecycle - from experimentation to '\n",
      "                    'evaluation - by combining all Azure AI services into a '\n",
      "                    'single hub with an handy GUI. The Model Catalog in Azure '\n",
      "                    'AI Studio enables the user to:\\n'\n",
      "                    '\\n'\n",
      "                    '- Find the Foundation Model of interest in the catalog - '\n",
      "                    'either proprietary or open source, filtering by task, '\n",
      "                    'license, or name. To improve searchability, the models '\n",
      "                    'are organized into collections, like Azure OpenAI '\n",
      "                    'collection, Hugging Face collection, and more.\\n'\n",
      "                    '\\n'\n",
      "                    '![Model '\n",
      "                    'catalog](./images/AzureAIStudioModelCatalog.png?WT.mc_id=academic-105485-koreyst)\\n'\n",
      "                    '\\n'\n",
      "                    '- Review the model card, including a detailed description '\n",
      "                    'of intended use and training data, code samples and '\n",
      "                    'evaluation results on internal evaluations library.\\n'\n",
      "                    '\\n'\n",
      "                    '![Model '\n",
      "                    'card](./images/ModelCard.png?WT.mc_id=academic-105485-koreyst)\\n'\n",
      "                    '\\n'\n",
      "                    '- Compare benchmarks across models and datasets available '\n",
      "                    'in the industry to assess which one meets the business '\n",
      "                    'scenario, through the [Model '\n",
      "                    'Benchmarks](https://learn.microsoft.com/azure/ai-studio/how-to/model-benchmarks?WT.mc_id=academic-105485-koreyst) '\n",
      "                    'pane.\\n'\n",
      "                    '\\n'\n",
      "                    '![Model '\n",
      "                    'benchmarks](./images/ModelBenchmarks.png?WT.mc_id=academic-105485-koreyst)\\n'\n",
      "                    '\\n'\n",
      "                    '- Fine-tune the model on custom training data to improve '\n",
      "                    'model performance in a specific workload, leveraging the '\n",
      "                    'experimentation and tracking capabilities of Azure AI '\n",
      "                    'Studio.\\n'\n",
      "                    '\\n'\n",
      "                    '![Model '\n",
      "                    'fine-tuning](./images/FineTuning.png?WT.mc_id=academic-105485-koreyst)\\n'\n",
      "                    '\\n'\n",
      "                    '- Deploy the original pre-trained model or the fine-tuned '\n",
      "                    'version to a remote real time inference - managed compute '\n",
      "                    '- or serverless api endpoint - '\n",
      "                    '[pay-as-you-go](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview#model-deployment-managed-compute-and-serverless-api-pay-as-you-go?WT.mc_id=academic-105485-koreyst) '\n",
      "                    '- to enable applications to consume it.\\n'\n",
      "                    '\\n'\n",
      "                    '![Model '\n",
      "                    'deployment](./images/ModelDeploy.png?WT.mc_id=academic-105485-koreyst)\\n'\n",
      "                    '\\n'\n",
      "                    '> [!NOTE]\\n'\n",
      "                    '> Not all models in the catalog are currently available '\n",
      "                    'for fine-tuning and/or pay-as-you-go deployment. Check '\n",
      "                    \"the model card for details on the model's capabilities \"\n",
      "                    'and limitations.\\n'\n",
      "                    '\\n',\n",
      "              'ko': '## Azure에서 성능을 이해하기 위해 다른 모델로 테스트하고 반복하는 방법\\n'\n",
      "                    '\\n'\n",
      "                    '팀은 현재 LLMs의 현황을 조사하고 시나리오에 적합한 몇 가지 좋은 후보 모델을 식별한 후, 다음 '\n",
      "                    '단계는 해당 데이터와 작업 부하에서 이들을 테스트하는 것입니다. 이는 실험과 측정을 통해 반복적으로 '\n",
      "                    '수행되는 과정입니다.\\n'\n",
      "                    '이전 단락에서 언급한 대부분의 모델들(OpenAI 모델, Llama2와 같은 오픈 소스 모델, '\n",
      "                    'Hugging Face transformers)은 [Azure Machine Learning '\n",
      "                    'studio](https://ml.azure.com/?WT.mc_id=academic-105485-koreyst)의 '\n",
      "                    '[Foundation '\n",
      "                    'Models](https://learn.microsoft.com/azure/machine-learning/concept-foundation-models?WT.mc_id=academic-105485-koreyst) '\n",
      "                    '카탈로그에서 사용할 수 있습니다.\\n'\n",
      "                    '\\n'\n",
      "                    '[Azure Machine '\n",
      "                    'Learning](https://azure.microsoft.com/products/machine-learning/?WT.mc_id=academic-105485-koreyst)은 '\n",
      "                    '데이터 과학자와 ML 엔지니어를 위해 설계된 클라우드 서비스로, ML 수명주기 전체(학습, 테스트, '\n",
      "                    '배포 및 MLOps 처리)를 단일 플랫폼에서 관리할 수 있도록 지원합니다. Machine '\n",
      "                    'Learning studio는 이 서비스에 대한 그래픽 사용자 인터페이스를 제공하며 사용자가 다음을 '\n",
      "                    '수행할 수 있도록 합니다:\\n'\n",
      "                    '\\n'\n",
      "                    '- 카탈로그에서 관심 있는 Foundation Model을 태스크, 라이선스 또는 이름으로 필터링하여 '\n",
      "                    '찾을 수 있습니다. 카탈로그에 아직 포함되지 않은 새로운 모델을 가져올 수도 있습니다.\\n'\n",
      "                    '- 상세한 설명과 코드 샘플을 포함한 모델 카드를 검토하고, 샘플 프롬프트를 제공하여 결과를 테스트하는 '\n",
      "                    '샘플 추론 위젯을 사용하여 모델을 테스트할 수 있습니다.\\n'\n",
      "                    '\\n'\n",
      "                    '![Model '\n",
      "                    'card](../../images/Llama1.png?WT.mc_id=academic-105485-koreyst)\\n'\n",
      "                    '\\n'\n",
      "                    '- 특정 작업 부하와 입력으로 제공된 특정 데이터 세트에 대한 객관적인 평가 메트릭을 사용하여 모델 '\n",
      "                    '성능을 평가할 수 있습니다.\\n'\n",
      "                    '\\n'\n",
      "                    '![Model '\n",
      "                    'evaluation](../../images/Llama2.png?WT.mc_id=academic-105485-koreyst)\\n'\n",
      "                    '\\n'\n",
      "                    '- Azure Machine Learning의 실험 및 추적 기능을 활용하여 특정 작업 부하에서 모델 '\n",
      "                    '성능을 개선하기 위해 사용자 지정 훈련 데이터로 모델을 세밀하게 조정할 수 있습니다.\\n'\n",
      "                    '\\n'\n",
      "                    '![Model '\n",
      "                    'fine-tuning](../../images/Llama3.png?WT.mc_id=academic-105485-koreyst)\\n'\n",
      "                    '\\n'\n",
      "                    '- 원래의 사전 훈련된 모델 또는 세밀하게 조정된 버전을 원격 실시간 추론 또는 배치 엔드포인트에 '\n",
      "                    '배포하여 응용 프로그램에서 사용할 수 있도록 할 수 있습니다.\\n'\n",
      "                    '\\n'\n",
      "                    '![Model '\n",
      "                    'deployment](../../images/Llama4.png?WT.mc_id=academic-105485-koreyst)\\n'\n",
      "                    '\\n'},\n",
      " 'score': 0.626947165,\n",
      " 'values': []}\n",
      "\n",
      "its query: {'id': '6-1',\n",
      " 'metadata': {'en': '## Introduction\\n'\n",
      "                    '\\n'\n",
      "                    'This lesson will cover:\\n'\n",
      "                    '\\n'\n",
      "                    '- Different types of LLMs in the current landscape.\\n'\n",
      "                    '- Testing, iterating, and comparing different models for '\n",
      "                    'your use case in Azure.\\n'\n",
      "                    '- How to deploy an LLM.\\n'\n",
      "                    '\\n',\n",
      "              'ko': '## 소개\\n'\n",
      "                    '\\n'\n",
      "                    '이 레슨에서는 다음을 다룰 것입니다:\\n'\n",
      "                    '\\n'\n",
      "                    '- 현재 풍경에서 다양한 유형의 LLM.\\n'\n",
      "                    '- Azure에서 사용 사례에 대한 다른 모델을 테스트, 반복 및 비교하는 방법.\\n'\n",
      "                    '- LLM을 배포하는 방법.\\n'\n",
      "                    '\\n'},\n",
      " 'score': 0.598735154,\n",
      " 'values': []}\n",
      "\n",
      "ChatCompletion(id='chatcmpl-ACeZAMKmOkdRQymKLPsNL68DglzeK', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='## 소개\\n\\n이 장에서는 다음을 이해할 것입니다:\\n\\n- MLOps에서 LLMOps로의 패러다임 전환\\n- LLM 수명 주기\\n- 수명 주기 도구\\n- 수명 주기 측정 및 평가', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1727579236, model='gpt-4o-2024-05-13', object='chat.completion', service_tier=None, system_fingerprint='fp_5796ac6771', usage=CompletionUsage(completion_tokens=54, prompt_tokens=204, total_tokens=258, completion_tokens_details=CompletionTokensDetails(reasoning_tokens=0)))\n",
      "ChatCompletion(id='chatcmpl-ACeZAM35KGw9DBdlVMNytHYPMsFgF', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='## 훌륭합니다! 학습을 계속하세요!\\n\\n놀랍습니다! 이제 [Contoso Chat App](https://nitya.github.io/contoso-chat/?WT.mc_id=academic-105485-koreyst)을 통해 애플리케이션을 구조화하여 개념을 사용하는 방법을 배우고, Cloud Advocacy가 이러한 개념을 데모에 추가하는 방식을 확인하세요. 더 많은 콘텐츠를 보려면 [Ignite 분과 세션](https://www.youtube.com/watch?v=DdOylyrTOWg)을 확인하세요!\\n\\n이제 Lesson 15를 확인하여 [Retrieval Augmented Generation 및 벡터 데이터베이스](../15-rag-and-vector-databases/README.md?WT.mc_id=academic-105485-koreyst)가 생성 AI에 미치는 영향과 보다 흥미로운 애플리케이션을 만드는 방법을 이해하세요!', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1727579236, model='gpt-4o-2024-05-13', object='chat.completion', service_tier=None, system_fingerprint='fp_057232b607', usage=CompletionUsage(completion_tokens=188, prompt_tokens=425, total_tokens=613, completion_tokens_details=CompletionTokensDetails(reasoning_tokens=0)))\n",
      "ChatCompletion(id='chatcmpl-ACeZAuyCgHDfvfGSbgosksV3ygVoL', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='# 생성형 AI 애플리케이션 생애주기\\n\\n모든 AI 애플리케이션에 중요한 질문은 AI 기능의 적절성입니다. AI는 빠르게 발전하는 분야이므로, 애플리케이션이 지속적으로 관련성, 신뢰성 및 강건성을 유지하도록 하기 위해서는 모니터링, 평가 및 개선이 계속 필요합니다. 이러한 과정에서 생성형 AI 생애주기가 중요한 역할을 합니다.\\n\\n생성형 AI 생애주기는 생성형 AI 애플리케이션을 개발, 배포 및 유지 관리하는 단계를 안내하는 프레임워크입니다. 이를 통해 목표를 설정하고, 성능을 측정하며, 도전에 대응하고, 해결책을 구현할 수 있습니다. 또한, 도메인 및 이해 관계자의 윤리적 및 법적 기준에 애플리케이션을 일치시킬 수 있도록 도와줍니다. 생성형 AI 생애주기를 따르면 애플리케이션이 항상 가치를 제공하고 사용자 만족도를 충족시킬 수 있습니다.', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1727579236, model='gpt-4o-2024-05-13', object='chat.completion', service_tier=None, system_fingerprint='fp_5796ac6771', usage=CompletionUsage(completion_tokens=221, prompt_tokens=1048, total_tokens=1269, completion_tokens_details=CompletionTokensDetails(reasoning_tokens=0)))\n",
      "ChatCompletion(id='chatcmpl-ACeZAnQSPzvNLMmoCuIFu1F9j4gVp', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='## MLOps에서 LLMOps로의 패러다임 전환 이해하기\\n\\nLLM은 인공지능 무기고의 새로운 도구로서, 응용 프로그램의 분석 및 생성 작업에 있어서 매우 강력합니다. 그러나 이 강력함은 AI와 고전적인 머신 러닝 작업을 간소화하는 방식에 몇 가지 영향을 미칩니다.\\n\\n이와 함께, 우리는 동적으로 이 도구를 적응시키기 위한 새로운 패러다임이 필요합니다. 적절한 인센티브로 말이죠. 이전의 AI 애플리케이션은 \"ML 애플리케이션\"으로, 새로운 AI 애플리케이션은 \"GenAI 애플리케이션\" 또는 단순히 \"AI 애플리케이션\"으로 분류할 수 있는데, 이는 당시에 사용된 주류 기술과 기법을 반영합니다. 이는 여러 방식으로 우리 내러티브를 변화시킵니다. 다음 비교를 살펴보세요.\\n\\n![LLMOps vs. MLOps 비교](./images/01-llmops-shift.png?WT.mc_id=academic-105485-koreys)\\n\\nLLMOps에서는 앱 개발자에 더욱 초점을 맞추고, 통합을 주요 포인트로 사용하며 \"서비스로서의 모델(Models-as-a-Service)\"을 사용하고 지표를 다음과 같이 생각합니다.\\n\\n- 품질: 응답 품질\\n- 해악: 책임 있는 AI\\n- 정직성: 응답의 타당성 (의미가 있는가? 정확한가?)\\n- 비용: 솔루션 예산\\n- 지연 시간: 평균 토큰 응답 시간', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1727579236, model='gpt-4o-2024-05-13', object='chat.completion', service_tier=None, system_fingerprint='fp_057232b607', usage=CompletionUsage(completion_tokens=353, prompt_tokens=707, total_tokens=1060, completion_tokens_details=CompletionTokensDetails(reasoning_tokens=0)))\n",
      "ChatCompletion(id='chatcmpl-ACeZAxmzFGiksFvTt04GolGJxOoKw', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='## 라이프사이클 도구\\n\\n도구에 관해서는, Microsoft는 [Azure AI Platform](https://azure.microsoft.com/solutions/ai/?WT.mc_id=academic-105485-koreys)과 [PromptFlow](https://microsoft.github.io/promptflow/index.html?WT.mc_id=academic-105485-koreyst) 제공하여 사용자가 쉽게 주기를 구현하고 실행할 수 있도록 합니다.\\n\\n[Azure AI Platform](https://azure.microsoft.com/solutions/ai/?WT.mc_id=academic-105485-koreys)은 [AI Studio](https://ai.azure.com/?WT.mc_id=academic-105485-koreys)를 사용할 수 있게 합니다. AI Studio는 다양한 모델, 샘플, 도구 등을 탐색할 수 있는 웹 포털입니다. 자원을 관리하고, UI 개발 흐름과 코드-중심 개발을 위한 SDK/CLI 옵션을 포함합니다.\\n\\n![Azure AI 가능성](./images/04-azure-ai-platform.png?WT.mc_id=academic-105485-koreys)\\n\\nAzure AI는 여러 자원을 사용하여 운영, 서비스, 프로젝트, 벡터 검색 및 데이터베이스 필요를 관리할 수 있게 합니다.\\n\\n![LLMOps with Azure AI](./images/05-llm-azure-ai-prompt.png?WT.mc_id=academic-105485-koreys)\\n\\nPromptFlow와 함께 개념 증명(Proof-of-Concept, POC)에서 대규모 애플리케이션까지 구축:\\n\\n- VS Code에서 시각적이고 기능적인 도구를 사용하여 앱을 설계하고 빌드\\n- 품질 높은 AI를 위해 앱을 쉽게 테스트하고 조정\\n- 클라우드와 통합하고 반복하기 위해 Azure AI Studio를 사용하고, 신속한 통합을 위해 Push 및 Deploy\\n\\n![LLMOps with PromptFlow](./images/06-llm-promptflow.png?WT.mc_id=academic-105485-koreys)\\n\\n', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1727579236, model='gpt-4o-2024-05-13', object='chat.completion', service_tier=None, system_fingerprint='fp_c17d3befe7', usage=CompletionUsage(completion_tokens=412, prompt_tokens=1676, total_tokens=2088, completion_tokens_details=CompletionTokensDetails(reasoning_tokens=0)))\n",
      "ChatCompletion(id='chatcmpl-ACeZ9rBvq3HARItHUA8ezrC3vED6P', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='## LLM 라이프사이클\\n\\n먼저, 라이프사이클과 그 변화를 이해하기 위해 다음 인포그래픽을 참고하세요.\\n\\n![LLMOps infographic](./images/02-llmops.png?WT.mc_id=academic-105485-koreys)\\n\\n보시다시피, 이것은 일반적인 MLOps 라이프사이클과 다릅니다. LLM은 프롬프트 엔지니어링, 품질 향상을 위한 다양한 기법 (Fine-Tuning, RAG, Meta-Prompts), 책임 있는 AI와의 평가 및 책임 등 여러 새로운 요구 사항이 있습니다. 마지막으로, 새로운 평가 지표 (품질, 해악, 정직성, 비용 및 지연)가 있습니다.\\n\\n예를 들어, 우리는 다양한 LLM을 사용하여 가설이 맞는지 테스트하기 위해 프롬프트 엔지니어링을 사용하여 아이디어를 구상합니다.\\n\\n이는 선형적이지 않으며, 통합된 루프, 반복적이며 전반적인 주기로 이루어집니다.\\n\\n이 단계를 탐색하려면 어떻게 해야 할까요? 라이프사이클을 구축하는 방법에 대해 자세히 살펴보겠습니다.\\n\\n![LLMOps Workflow](./images/03-llm-stage-flows.png?WT.mc_id=academic-105485-koreys)\\n\\n이것은 약간 복잡해 보일 수 있습니다. 먼저 세 가지 큰 단계에 집중해 보겠습니다.\\n\\n1. 아이디어 구상/탐색: 탐색 단계입니다. 여기에서 비즈니스 요구에 따라 탐색할 수 있습니다. 프로토타이핑, [PromptFlow](https://microsoft.github.io/promptflow/index.html?WT.mc_id=academic-105485-koreyst) 생성 및 가설이 충분히 효율적인지 테스트합니다.\\n1. 구축/증강: 구현 단계입니다. 이제 더 큰 데이터 세트를 평가하고 Fine-Tuning 및 RAG와 같은 기법을 사용하여 솔루션의 견고성을 확인합니다. 해결되지 않으면, 흐름에 새로운 단계를 추가하거나 데이터를 재구조화하는 등의 재구현이 도움이 될 수 있습니다. 흐름과 확장을 테스트하여 작동하고 지표가 만족스러우면 다음 단계로 넘어갈 준비가 된 것입니다.\\n1. 운영화: 통합 단계입니다. 이제 모니터링 및 경고 시스템을 시스템에 추가하고 애플리케이션에 배포하고 통합합니다.\\n\\n그런 다음 보안, 컴플라이언스 및 거버넌스를 중심으로 관리 주기가 있습니다.\\n\\n축하합니다, 이제 AI 애플리케이션이 준비되어 운영 가능합니다. 실습 경험을 원한다면, [Contoso Chat 데모](https://nitya.github.io/contoso-chat/?WT.mc_id=academic-105485-koreys)를 참조하세요.\\n\\n이제 어떤 도구를 사용할 수 있을까요?', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1727579235, model='gpt-4o-2024-05-13', object='chat.completion', service_tier=None, system_fingerprint='fp_057232b607', usage=CompletionUsage(completion_tokens=617, prompt_tokens=1309, total_tokens=1926, completion_tokens_details=CompletionTokensDetails(reasoning_tokens=0)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "from openai import AsyncOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "client = AsyncOpenAI()\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "\n",
    "\n",
    "# 1. 검색어 임베딩으로 변환\n",
    "async def to_embedding(text):\n",
    "    return (\n",
    "        (\n",
    "            await client.embeddings.create(\n",
    "                model=\"text-embedding-3-small\",\n",
    "                input=text,\n",
    "                encoding_format=\"float\",\n",
    "            )\n",
    "        )\n",
    "        .data[0]\n",
    "        .embedding\n",
    "    )\n",
    "\n",
    "\n",
    "# 2. 임베딩된 검색어 기존 Pinecone DB 에서 검색 (가장 유사도 높은것 1개)\n",
    "def query_embedding(vector):\n",
    "    index = pc.Index(\"gen-ai\")\n",
    "    return index.query(\n",
    "        namespace=\"ns1\",\n",
    "        vector=vector,\n",
    "        top_k=1,\n",
    "        include_values=False,\n",
    "        include_metadata=True,\n",
    "    )[\"matches\"][0]\n",
    "\n",
    "\n",
    "async def translate_text(text):\n",
    "    print(f\"Searching for translation of: {text}...\\n\")\n",
    "    embedding = await to_embedding(text)\n",
    "    query = query_embedding(embedding)\n",
    "    print(f\"its query: {query}\\n\")\n",
    "    # 3. 쿼리 결과를 컨텍스트로 넘기고, 번역\n",
    "    completion = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an AI expert. Your job is to translate English markdown to Korean markdown. Output should be a markdown string that can be saved to a file. Omit ```\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": f\"\"\"Based on the previous translation: \n",
    "                English: {query[\"metadata\"][\"en\"]}\n",
    "                Korean: {query[\"metadata\"][\"ko\"]}\n",
    "            \"\"\",\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    print(completion)\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "async def translate_file(file, i):\n",
    "    tasks = [translate_text(section) for section in file[\"sections\"]]\n",
    "    translated_text = await asyncio.gather(*tasks)\n",
    "    content = \"\\n\\n\".join(translated_text)\n",
    "    save_to_file(f\"{i:02d}-{file['title']}/translations/ko/README.md\", content)\n",
    "\n",
    "\n",
    "tasks = [translate_file(file, i) for i, file in list(readme_files.items())]\n",
    "await asyncio.gather(*tasks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
